default:
  max_steps: 20000
  data_dir: ~/machine
  batch_size: 1024
  huggingface:
    parent_model_name: facebook/nllb-200-1.3B
    train_params:
      do_train: true
      optim: adamw_torch
      warmup_steps: 4000
      per_device_train_batch_size: 16
      gradient_accumulation_steps: 4
      label_smoothing_factor: 0.2
      group_by_length: true
      gradient_checkpointing: true
      fp16: true
      save_strategy: no
    generate_params:
      device: 0
      num_beams: 2
      batch_size: 16
development:
  shared_file_uri: s3://aqua-ml-data/
  huggingface:
    parent_model_name: facebook/nllb-200-distilled-600M
    generate_params:
      num_beams: 1
