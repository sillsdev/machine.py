default:
  model_type: huggingface
  data_dir: ~/machine
  huggingface:
    max_steps: 20000
    batch_size: 1024
    parent_model_name: facebook/nllb-200-distilled-1.3B
    train_params:
      do_train: true
      optim: adamw_torch
      warmup_steps: 4000
      per_device_train_batch_size: 16
      gradient_accumulation_steps: 4
      label_smoothing_factor: 0.2
      group_by_length: true
      gradient_checkpointing: true
      fp16: true
      save_strategy: no
    generate_params:
      device: 0
      num_beams: 2
      batch_size: 16
      oom_batch_size_backoff_mult: 0.5
    tokenizer:
      add_unk_src_tokens: true
      add_unk_trg_tokens: true
development:
  shared_file_uri: s3://aqua-ml-data/dev/
  huggingface:
    parent_model_name: facebook/nllb-200-distilled-600M
    generate_params:
      num_beams: 1
staging:
  huggingface:
    max_steps: 10
    parent_model_name: facebook/nllb-200-distilled-600M
    generate_params:
      num_beams: 1