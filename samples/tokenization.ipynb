{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Tutorial\n",
    "\n",
    "There are many NLP methods that require tokenized data as input, such as machine translation and word alignment. In this notebook, we will show how to use the different tokenizers and detokenizers that are available in Machine. Tokenizers implement either the `Tokenizer` abstract class or the `RangeTokenizer` abstract class. `Tokenizer` classes are used to segment a sequence into tokens. `RangeTokenizer` classes return ranges that mark where each each token occurs in the sequence. Detokenizers implement the `Detokenizer` abstract class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing text\n",
    "\n",
    "Let's start with a simple, whitespace tokenizer. This tokenizer is used to split a string at whitespace. This tokenizer is useful for text that has already been tokenized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This | is | a | test | .\n"
     ]
    }
   ],
   "source": [
    "from machine.tokenization import WhitespaceTokenizer\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(\"This is a test .\")\n",
    "print(\" | \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine contains general tokenizers that can be used to tokenize text from languages with a Latin-based script. A word tokenizer and a sentence tokenizer are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer | scelerisque | efficitur | dui | , | eu | tincidunt | erat | posuere | in | .\n",
      "Curabitur | vel | finibus | mi | .\n"
     ]
    }
   ],
   "source": [
    "from machine.tokenization import LatinSentenceTokenizer, LatinWordTokenizer\n",
    "\n",
    "sentence_tokenizer = LatinSentenceTokenizer()\n",
    "sentences = sentence_tokenizer.tokenize(\n",
    "    \"Integer scelerisque efficitur dui, eu tincidunt erat posuere in. Curabitur vel finibus mi.\"\n",
    ")\n",
    "word_tokenizer = LatinWordTokenizer()\n",
    "print(\"\\n\".join(\" | \".join(word_tokenizer.tokenize(sentence)) for sentence in sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most tokenizers implement the `RangeTokenizer` interface. These tokenizers have an additional method, `tokenize_as_ranges`, that returns ranges that mark the position of all tokens in the original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"][This] [is] [a] [test][,] [also][.][\"]\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = LatinWordTokenizer()\n",
    "sentence = '\"This is a test, also.\"'\n",
    "ranges = word_tokenizer.tokenize_as_ranges(sentence)\n",
    "output = \"\"\n",
    "prev_end = 0\n",
    "for range in ranges:\n",
    "    output += sentence[prev_end : range.start]\n",
    "    output += f\"[{sentence[range.start : range.end]}]\"\n",
    "    prev_end = range.end\n",
    "print(output + sentence[prev_end:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some languages that do not delimit words with spaces, but instead delimit sentences with spaces. In these cases, it is common practice to use zero-width spaces to explicitly mark word boundaries. This is often done for Bible translations. Machine contains a word tokenizer that is designed to properly deal with text use zero-width space to delimit words and spaces to delimit sentences. Notice that the space is preserved, since it is being used as punctuation to delimit sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem | Ipsum | Dolor | Sit | Amet | Consectetur |   | Adipiscing | Elit | Sed\n"
     ]
    }
   ],
   "source": [
    "from machine.tokenization import ZwspWordTokenizer\n",
    "\n",
    "word_tokenizer = ZwspWordTokenizer()\n",
    "tokens = word_tokenizer.tokenize(\"Lorem​Ipsum​Dolor​Sit​Amet​Consectetur Adipiscing​Elit​Sed\")\n",
    "print(\" | \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subword tokenization has become popular for use with deep learning models. Machine provides a [SentencePiece](https://github.com/google/sentencepiece) tokenizer that can perform both BPE and unigram subword tokenization. Another advantage of subword tokenization is that it is language-independent and allows one to specify the size of the vocabulary. This helps to deal with out-of-vocabulary issues. First, let's train a SentencePiece model. SentencePiece classes require the `sentencepiece` optional dependency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as sp\n",
    "\n",
    "os.makedirs(\"out\", exist_ok=True)\n",
    "sp.SentencePieceTrainer.Train(f\"--input=data/en.txt --model_prefix=out/en-sp --vocab_size=400\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a SentencePiece model, we can split the text into subwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁Th | is | ▁ | is | ▁a | ▁ | t | es | t | .\n"
     ]
    }
   ],
   "source": [
    "from machine.tokenization.sentencepiece import SentencePieceTokenizer\n",
    "\n",
    "tokenizer = SentencePieceTokenizer(\"out/en-sp.model\")\n",
    "tokens = tokenizer.tokenize(\"This is a test.\")\n",
    "print(\" | \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detokenizing text\n",
    "\n",
    "For many NLP pipelines, tokens will need to be merged back into detokenized text. This is very common for machine translation. Many of the tokenizers in Machine also have a corresponding detokenizer that can be used to convert tokens back into a correct sequence. Once again, let's start with a simple, whitespace detokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test .\n"
     ]
    }
   ],
   "source": [
    "from machine.tokenization import WhitespaceDetokenizer\n",
    "\n",
    "detokenizer = WhitespaceDetokenizer()\n",
    "sentence = detokenizer.detokenize([\"This\", \"is\", \"a\", \"test\", \".\"])\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine has a general detokenizer that works well with languages with a Latin-based script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This is a test, also.\"\n"
     ]
    }
   ],
   "source": [
    "from machine.tokenization import LatinWordDetokenizer\n",
    "\n",
    "detokenizer = LatinWordDetokenizer()\n",
    "sentence = detokenizer.detokenize(['\"', \"This\", \"is\", \"a\", \"test\", \",\", \"also\", \".\", '\"'])\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine has a detokenizer that properly deals with text that uses zero-width space to delimit words and spaces to delimit sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem​Ipsum​Dolor​Sit​Amet​Consectetur Adipiscing​Elit​Sed\n"
     ]
    }
   ],
   "source": [
    "from machine.tokenization import ZwspWordDetokenizer\n",
    "\n",
    "word_detokenizer = ZwspWordDetokenizer()\n",
    "sentence = word_detokenizer.detokenize(\n",
    "    [\"Lorem\", \"Ipsum\", \"Dolor\", \"Sit\", \"Amet\", \"Consectetur\", \" \", \"Adipiscing\", \"Elit\", \"Sed\"]\n",
    ")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine contains a detokenizer for SentencePiece encoded text. SentencePiece encodes spaces in the tokens, so that it can be detokenized without any ambiguities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test.\n"
     ]
    }
   ],
   "source": [
    "from machine.tokenization.sentencepiece import SentencePieceDetokenizer\n",
    "\n",
    "detokenizer = SentencePieceDetokenizer()\n",
    "sentence = detokenizer.detokenize([\"▁Th\", \"is\", \"▁\", \"is\", \"▁a\", \"▁\", \"t\", \"es\", \"t\", \".\"])\n",
    "print(sentence)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5c3594217c843d98647b94b941817aced8bb402b4624bfdbaebf1a3617786b5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('sil-machine-hNQycPie-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
