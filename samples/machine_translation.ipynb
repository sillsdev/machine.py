{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation Tutorial\n",
    "\n",
    "Machine provides a general framework for machine translation engines. It currently provides implementations for statistical MT (SMT) and neural MT (NMT). All MT engines implement the same interfaces, which provides a high level of extensibility for calling applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sil-machine[thot]\n",
    "%pip install transformers==4.34.0 datasets sacremoses accelerate==0.26.1 gcsfs==2023.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sillsdev/machine.py.git\n",
    "%cd machine.py/samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Machine Translation\n",
    "\n",
    "Machine provides a phrase-based statistical machine translation engine that is based on the [Thot](https://github.com/sillsdev/thot) library. The SMT engine implemented in Thot is unique, because it supports incremental training and interactive machine translation (IMT). Let's start by training an SMT model. MT models implement the `TranslationModel` interface. SMT models are trained using a parallel text corpus, so the first step is to create a `ParallelTextCorpus`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import TextFileTextCorpus\n",
    "\n",
    "source_corpus = TextFileTextCorpus(\"data/sp.txt\")\n",
    "target_corpus = TextFileTextCorpus(\"data/en.txt\")\n",
    "parallel_corpus = source_corpus.align_rows(target_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainers are responsible for training MT models. A trainer can be created either using the constructor or using the `create_trainer` method on the `TranslationModel` interface. Creating a trainer by constructor is useful if you are training a new model. The `create_trainer` method is useful when you are retraining an existing model. In this example, we are going to construct the trainer directly. Word alignment is at the core of SMT. In this example, we are going to use HMM for word alignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... done.\n",
      "Saving model... done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from machine.tokenization import LatinWordTokenizer\n",
    "from machine.translation.thot import ThotSmtModelTrainer, ThotWordAlignmentModelType\n",
    "\n",
    "tokenizer = LatinWordTokenizer()\n",
    "os.makedirs(\"out/sp-en-smt\", exist_ok=True)\n",
    "shutil.copy(\"data/smt.cfg\", \"out/sp-en-smt/smt.cfg\")\n",
    "with ThotSmtModelTrainer(\n",
    "    ThotWordAlignmentModelType.HMM,\n",
    "    parallel_corpus,\n",
    "    \"out/sp-en-smt/smt.cfg\",\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    lowercase_source=True,\n",
    "    lowercase_target=True,\n",
    ") as trainer:\n",
    "    print(\"Training model...\", end=\"\")\n",
    "    trainer.train()\n",
    "    print(\" done.\")\n",
    "    print(\"Saving model...\", end=\"\")\n",
    "    trainer.save()\n",
    "    print(\" done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fully translate a sentence, we need to perform pre-processing steps on the source sentence and post-processing steps on the target translation. Here are the steps to fully translate a sentence:\n",
    "\n",
    "1. Tokenize the source sentence.\n",
    "2. Lowercase the source tokens.\n",
    "3. Translate the sentence.\n",
    "4. Truecase the target tokens.\n",
    "5. Detokenize the target tokens into a sentence.\n",
    "\n",
    "Truecasing is the process of properly capitalizing a lowercased sentence. Luckily, Machine provides a statistical truecaser that can learn the capitalization rules for a language. The next step is train the truecaser model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.translation import UnigramTruecaserTrainer\n",
    "\n",
    "with UnigramTruecaserTrainer(\"out/sp-en-smt/en.truecase.txt\", target_corpus, tokenizer=tokenizer) as trainer:\n",
    "    trainer.train()\n",
    "    trainer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained SMT model and a trained truecasing model, we are ready to translate sentences. First, We need to load the SMT model. The model can be used to translate sentences using the `translate` method. A `TranslationResult` instance is returned when a text segment is translated. In addition to the translated segment, `TranslationResult` contains lots of interesting information about the translated sentence, such as the word confidences, alignment, phrases, and source/target tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: I would like to book a room until tomorrow.\n",
      "Source tokens: ['Desearía', 'reservar', 'una', 'habitación', 'hasta', 'mañana', '.']\n",
      "Target tokens: ['I', 'would', 'like', 'to', 'book', 'a', 'room', 'until', 'tomorrow', '.']\n",
      "Alignment: 0-1 0-2 1-3 1-4 2-5 3-6 4-7 5-8 6-9\n",
      "Confidences: [0.1833474940416596, 0.3568307371510516, 0.3556863860951534, 0.2894564705698258, 0.726984900023586, 0.8915912178040876, 0.878754356224247, 0.8849444691927844, 0.8458962922106739, 0.8975745812873857]\n"
     ]
    }
   ],
   "source": [
    "from machine.translation import UnigramTruecaser\n",
    "from machine.translation.thot import ThotSmtModel\n",
    "from machine.tokenization import LatinWordDetokenizer\n",
    "\n",
    "truecaser = UnigramTruecaser(\"out/sp-en-smt/en.truecase.txt\")\n",
    "detokenizer = LatinWordDetokenizer()\n",
    "\n",
    "with ThotSmtModel(\n",
    "    ThotWordAlignmentModelType.HMM,\n",
    "    \"out/sp-en-smt/smt.cfg\",\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    target_detokenizer=detokenizer,\n",
    "    truecaser=truecaser,\n",
    "    lowercase_source=True,\n",
    "    lowercase_target=True,\n",
    ") as model:\n",
    "    result = model.translate(\"Desearía reservar una habitación hasta mañana.\")\n",
    "    print(\"Translation:\", result.translation)\n",
    "    print(\"Source tokens:\", result.source_tokens)\n",
    "    print(\"Target tokens:\", result.target_tokens)\n",
    "    print(\"Alignment:\", result.alignment)\n",
    "    print(\"Confidences:\", result.confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Machine Translation\n",
    "\n",
    "`ThotSmtModel` also supports interactive machine translation. Under this paradigm, the engine assists a human translator by providing translations suggestions based on what the user has translated so far. This paradigm can be coupled with incremental training to provide a model that is constantly learning from translator input. Models and engines must implement the `InteractiveTranslationModel` and `InteractiveTranslationEngine` interfaces to support IMT. The IMT paradigm is implemented in the `InteractiveTranslator` class. The `approve` method on `InteractiveTranslator` performs incremental training using the current prefix. Suggestions are generated from translations using a class that implements the `TranslationSuggester` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Hablé con recepción.\n",
      "Suggestion: [With reception]\n",
      "Suggestion: I spoke [with reception]\n",
      "Suggestion: I spoke with reception. []\n",
      "\n",
      "Source: Hablé hasta cinco en punto.\n",
      "Suggestion: [I spoke until five o'clock]\n",
      "Suggestion: I spoke until five o'clock. []\n"
     ]
    }
   ],
   "source": [
    "from machine.translation import PhraseTranslationSuggester, InteractiveTranslatorFactory\n",
    "\n",
    "suggester = PhraseTranslationSuggester()\n",
    "\n",
    "def get_current_suggestion(translator):\n",
    "    suggestion = next(iter(suggester.get_suggestions_from_translator(1, translator)), None)\n",
    "    suggestion_text = \"\" if suggestion is None else detokenizer.detokenize(suggestion.target_words)\n",
    "    if len(translator.prefix) == 0:\n",
    "        suggestion_text = suggestion_text.capitalize()\n",
    "    prefix_text = translator.prefix.strip()\n",
    "    if len(prefix_text) > 0:\n",
    "        prefix_text = prefix_text + \" \"\n",
    "    return f\"{prefix_text}[{suggestion_text}]\"\n",
    "\n",
    "\n",
    "with ThotSmtModel(\n",
    "    ThotWordAlignmentModelType.HMM,\n",
    "    \"out/sp-en-smt/smt.cfg\",\n",
    "    source_tokenizer=tokenizer,\n",
    "    target_tokenizer=tokenizer,\n",
    "    target_detokenizer=detokenizer,\n",
    "    truecaser=truecaser,\n",
    "    lowercase_source=True,\n",
    "    lowercase_target=True,\n",
    ") as model:\n",
    "    factory = InteractiveTranslatorFactory(model, target_tokenizer=tokenizer, target_detokenizer=detokenizer)\n",
    "\n",
    "    source_sentence = \"Hablé con recepción.\"\n",
    "    print(\"Source:\", source_sentence)\n",
    "    translator = factory.create(source_sentence)\n",
    "\n",
    "    suggestion = get_current_suggestion(translator)\n",
    "    print(\"Suggestion:\", suggestion)\n",
    "\n",
    "    translator.append_to_prefix(\"I spoke \")\n",
    "    suggestion = get_current_suggestion(translator)\n",
    "    print(\"Suggestion:\", suggestion)\n",
    "\n",
    "    translator.append_to_prefix(\"with reception.\")\n",
    "    suggestion = get_current_suggestion(translator)\n",
    "    print(\"Suggestion:\", suggestion)\n",
    "    translator.approve(aligned_only=False)\n",
    "    print()\n",
    "\n",
    "    source_sentence = \"Hablé hasta cinco en punto.\"\n",
    "    print(\"Source:\", source_sentence)\n",
    "    translator = factory.create(source_sentence)\n",
    "\n",
    "    suggestion = get_current_suggestion(translator)\n",
    "    print(\"Suggestion:\", suggestion)\n",
    "\n",
    "    translator.append_to_prefix(\"I spoke until five o'clock.\")\n",
    "    suggestion = get_current_suggestion(translator)\n",
    "    print(\"Suggestion:\", suggestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation\n",
    "\n",
    "Machine also supports neural machine translation through the use of the Huggingface [Transformers](https://huggingface.co/docs/transformers/en/index) library. The Huggingface NMT engine implements the same interfaces that the SMT engine does, so you can train and inference the engine using the same API.\n",
    "\n",
    "Let's start by fine tuning an NMT model using `HuggingFaceNmtModelTrainer`. One thing to note is that Huggingface models typically have an associated tokenizer. The trainer will handle tokenization for us, so we don't have to tokenize the corpus. We will need to specify the base model and the training arguments. For this example, we will be fine tuning an M2M100 model. We will also need to specify the source and target languages according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore transformers warnings\n",
    "import warnings\n",
    "from transformers.utils import logging as transformers_logging\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d72fb4ece0e4f60a\n",
      "Found cached dataset generator (C:/Users/damie/.cache/huggingface/datasets/generator/default-d72fb4ece0e4f60a/0.0.0)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011031389236450195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Running tokenizer on train dataset",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ea4c3fa51e4b0787da28d64f291491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011000871658325195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 125,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980bad4b92844b8daa68042e4e610320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 435.3667, 'train_samples_per_second': 2.297, 'train_steps_per_second': 0.287, 'train_loss': 0.27040533447265624, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.2704\n",
      "  train_runtime            = 0:07:15.36\n",
      "  train_samples            =       1000\n",
      "  train_samples_per_second =      2.297\n",
      "  train_steps_per_second   =      0.287\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from machine.translation.huggingface import HuggingFaceNmtModelTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"out/sp-en-nmt\", overwrite_output_dir=True, num_train_epochs=1, report_to=[], disable_tqdm=False\n",
    ")\n",
    "\n",
    "with HuggingFaceNmtModelTrainer(\n",
    "    \"facebook/m2m100_418M\",\n",
    "    training_args,\n",
    "    parallel_corpus,\n",
    "    src_lang=\"es\",\n",
    "    tgt_lang=\"en\",\n",
    "    add_unk_src_tokens=False,\n",
    "    add_unk_tgt_tokens=False,\n",
    ") as trainer:\n",
    "    trainer.train()\n",
    "    trainer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, let's try using it to translate some sentences. We need to use `HuggingFaceNmtEngine` to load the model and perform inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: I would like to book a room until tomorrow.\n",
      "Source tokens: ['▁D', 'ese', 'aría', '▁res', 'ervar', '▁una', '▁hab', 'itación', '▁hasta', '▁mañana', '.']\n",
      "Target tokens: ['▁I', '▁would', '▁like', '▁to', '▁book', '▁a', '▁room', '▁until', '▁tom', 'orrow', '.']\n",
      "Alignment: 1-2 2-0 2-1 3-4 4-3 5-5 6-6 8-7 9-8 9-9 10-10\n",
      "Confidences: [0.9995167207904968, 0.9988614185814005, 0.9995524502931971, 0.9861009574421602, 0.9987220427038153, 0.998968593209302, 0.9944791909715244, 0.9989702587912649, 0.9749540518542505, 0.9996603689253716, 0.9930446924545876]\n"
     ]
    }
   ],
   "source": [
    "from machine.translation.huggingface import HuggingFaceNmtEngine\n",
    "\n",
    "with HuggingFaceNmtEngine(\"out/sp-en-nmt\", src_lang=\"es\", tgt_lang=\"en\") as engine:\n",
    "    result = engine.translate(\"Desearía reservar una habitación hasta mañana.\")\n",
    "    print(\"Translation:\", result.translation)\n",
    "    print(\"Source tokens:\", result.source_tokens)\n",
    "    print(\"Target tokens:\", result.target_tokens)\n",
    "    print(\"Alignment:\", result.alignment)\n",
    "    print(\"Confidences:\", result.confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform inferencing on a pretrained Huggingface model without fine tuning. Let's translate the same sentence using NLLB-200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: I'd like to reserve a room for tomorrow.\n",
      "Source tokens: ['▁Dese', 'aría', '▁reser', 'var', '▁una', '▁habitación', '▁hasta', '▁mañana', '.']\n",
      "Target tokens: ['▁I', \"'\", 'd', '▁like', '▁to', '▁reserve', '▁a', '▁room', '▁for', '▁tomorrow', '.']\n",
      "Alignment: 0-1 0-3 1-0 1-2 2-5 5-6 5-7 6-8 7-9 8-4 8-10\n",
      "Confidences: [0.766540320750896, 0.5910241514763206, 0.8868627789322919, 0.8544048979056736, 0.8613305047447863, 0.45655845183164, 0.8814725030368357, 0.8585703155792751, 0.3142652857171965, 0.8780149028315941, 0.8617016651426532]\n"
     ]
    }
   ],
   "source": [
    "with HuggingFaceNmtEngine(\"facebook/nllb-200-distilled-600M\", src_lang=\"spa_Latn\", tgt_lang=\"eng_Latn\") as engine:\n",
    "    result = engine.translate(\"Desearía reservar una habitación hasta mañana.\")\n",
    "    print(\"Translation:\", result.translation)\n",
    "    print(\"Source tokens:\", result.source_tokens)\n",
    "    print(\"Target tokens:\", result.target_tokens)\n",
    "    print(\"Alignment:\", result.alignment)\n",
    "    print(\"Confidences:\", result.confidences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sil-machine-7etMAqKM-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
