{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Alignment Tutorial\n",
    "\n",
    "In this notebook, we will demonstrate how to use machine to train statistical word alignment models and then use them to predict alignments between sentences. Machine uses the [Thot](https://github.com/sillsdev/thot) library to implement word alignment models. The classes can be enabled by installing the `sil-machine` package with the `thot` optional dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models\n",
    "\n",
    "The first step in training a statistical word alignment model is setting up a parallel text corpus. Word alignment models are unsupervised, so they only require a parallel text corpus to train. Manually created alignments are not necessary. So let's create a `ParallelTextCorpus` instance from the source and target monolingual corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine.corpora import ParallelTextCorpus, ParatextTextCorpus\n",
    "\n",
    "source_corpus = ParatextTextCorpus(\"data/VBL-PT\")\n",
    "target_corpus = ParatextTextCorpus(\"data/WEB-PT\")\n",
    "parallel_corpus = ParallelTextCorpus(source_corpus, target_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine has implementations of all common statistical models, including the famous IBM models (1-4), HMM, and FastAlign. All alignment models implement the `WordAlignmentModel` abstract base class. This makes it easier to swap out different models in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to start by training an IBM-1 model. There are two possible ways to train a model. First, we will demonstrate training a model from a class that inherits from `WordAlignmentModel`. We use the `create_trainer` method to create a trainer object that is used to train the model. If we do not specify a file path when creating the model object, then the model will only exist in memory. When we call the `save` method, the model instance is updated with the trained model parameters, but the model is not written to disk. We need to preprocess the corpus before training. First, we need to tokenize the corpus. We will also lowercase the data, since that generally gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IBM-1 model: 0.00%\n",
      "Training IBM-1 model: 16.67%\n",
      "Training IBM-1 model: 33.33%\n",
      "Training IBM-1 model: 50.00%\n",
      "Training IBM-1 model: 66.67%\n",
      "Training IBM-1 model: 83.33%\n",
      "Training IBM-1 model: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from machine.translation.thot import ThotIbm1WordAlignmentModel\n",
    "from machine.tokenization import LatinWordTokenizer\n",
    "\n",
    "tokenizer = LatinWordTokenizer()\n",
    "parallel_corpus = parallel_corpus.tokenize(tokenizer).lowercase()\n",
    "\n",
    "model = ThotIbm1WordAlignmentModel()\n",
    "trainer = model.create_trainer(parallel_corpus)\n",
    "trainer.train(lambda status: print(f\"Training IBM-1 model: {status.percent_completed:.2%}\"))\n",
    "trainer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other option for training a model is to construct a trainer object directly. This method is useful for when you are only interested in training the model and saving it to disk for later use. We need to specify where the model will be saved after it is trained and we call the `save` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IBM-1 model: 0.00%\n",
      "Training IBM-1 model: 16.67%\n",
      "Training IBM-1 model: 33.33%\n",
      "Training IBM-1 model: 50.00%\n",
      "Training IBM-1 model: 66.67%\n",
      "Training IBM-1 model: 83.33%\n",
      "Training IBM-1 model: 100.00%\n",
      "IBM-1 model saved\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from machine.translation.thot import ThotWordAlignmentModelTrainer, ThotWordAlignmentModelType\n",
    "\n",
    "os.makedirs(\"out/VBL-WEB-IBM1\", exist_ok=True)\n",
    "trainer = ThotWordAlignmentModelTrainer(ThotWordAlignmentModelType.IBM1, parallel_corpus, \"out/VBL-WEB-IBM1/src_trg\")\n",
    "\n",
    "trainer.train(lambda status: print(f\"Training IBM-1 model: {status.percent_completed:.2%}\"))\n",
    "trainer.save()\n",
    "print(\"IBM-1 model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning parallel sentences\n",
    "\n",
    "Now that we have a trained alignment model, we can find the best alignment for a parallel sentence. We call `get_best_alignment` method to find the best alignment. The results are returned as a `WordAlignmentMatrix` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1JN 1:1\n",
      "Source: esta carta trata sobre la palabra de vida que existía desde el principio , que hemos escuchado , que hemos visto con nuestros propios ojos y le hemos contemplado , y que hemos tocado con nuestras manos .\n",
      "Target: that which was from the beginning , that which we have heard , that which we have seen with our eyes , that which we saw , and our hands touched , concerning the word of life\n",
      "Alignment: 15-0 16-1 2-2 12-3 4-4 12-5 13-6 15-7 16-8 35-9 15-10 16-11 13-12 15-13 16-14 35-15 15-16 20-17 21-18 22-19 2-20 13-21 15-22 16-23 35-24 2-25 13-26 25-27 22-28 2-29 2-30 13-31 3-32 4-33 5-34 6-35 7-36\n",
      "1JN 1:2\n",
      "Source: esta vida nos fue revelada . la vimos y damos testimonio de ella . estamos hablándoles de aquél que es la vida eterna , que estaba con el padre , y que nos fue revelado .\n",
      "Target: ( and the life was revealed , and we have seen , and testify , and declare to you the life , the eternal life , which was with the father , and was revealed to us ) ;\n",
      "Alignment: 4-0 8-1 28-2 1-3 3-4 34-5 23-6 8-7 14-8 28-9 4-10 23-11 8-12 4-13 23-14 8-15 4-16 0-17 12-18 28-19 1-20 23-21 28-22 22-23 1-24 23-25 10-26 3-27 26-28 28-29 28-30 23-31 8-32 3-33 34-34 0-35 2-36 4-37 34-38\n",
      "1JN 1:3\n",
      "Source: los que hemos visto y oído eso mismo les contamos , para que también puedan participar de esta amistad junto a nosotros . esta amistad con el padre y su hijo jesucristo .\n",
      "Target: that which we have seen and heard we declare to you , that you also may have fellowship with us . yes , and our fellowship is with the father and with his son , jesus christ .\n",
      "Alignment: 2-0 2-1 2-2 2-3 3-4 4-5 18-6 2-7 18-8 6-9 8-10 10-11 2-12 8-13 13-14 14-15 2-16 18-17 25-18 21-19 22-20 5-21 10-22 4-23 18-24 18-25 26-26 25-27 27-28 27-29 4-30 25-31 29-32 30-33 10-34 31-35 31-36 22-37\n",
      "1JN 1:4\n",
      "Source: escribimos para decirles esto , a fin de que nuestra felicidad sea completa .\n",
      "Target: and we write these things to you , that our joy may be fulfilled .\n",
      "Alignment: 6-0 12-1 6-2 2-3 2-4 3-5 3-6 4-7 1-8 0-9 0-10 12-11 9-12 0-13 13-14\n",
      "1JN 1:5\n",
      "Source: este es el mensaje que recibimos de él y que nosotros les declaramos a ustedes : dios es luz , y no hay ningún vestigio de oscuridad en él .\n",
      "Target: this is the message which we have heard from him and announce to you , that god is light , and in him is no darkness at all .\n",
      "Alignment: 15-0 1-1 2-2 3-3 3-4 10-5 14-6 3-7 0-8 7-9 8-10 5-11 22-12 14-13 19-14 4-15 16-16 1-17 18-18 19-19 8-20 27-21 7-22 1-23 22-24 26-25 5-26 5-27 29-28\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "model = ThotIbm1WordAlignmentModel(\"out/VBL-WEB-IBM1/src_trg\")\n",
    "for row in islice(parallel_corpus.get_rows(), 5):\n",
    "    alignment = model.get_best_alignment(row.source_segment, row.target_segment)\n",
    "\n",
    "    print(row.ref)\n",
    "    print(\"Source:\", row.source_text)\n",
    "    print(\"Target:\", row.target_text)\n",
    "    print(\"Alignment:\", str(alignment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting model probabilities\n",
    "\n",
    "A statistical word alignment model consists of one or more conditional probability distributions that are estimated from the training data. For example, most models estimate a word translation probability distribution that can be queried to obtain the probability that a source word is a translation of a target word. Each model class has methods to obtain these probabilities. Let's try getting some translation probabilities from the IBM-1 model that we trained by calling the `get_translation_probability` method. In order to get the probability that a word does not translate to anything, you can pass `None` instead of the word string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es -> is: 0.2720\n",
      "NULL -> that: 0.0516\n"
     ]
    }
   ],
   "source": [
    "model = ThotIbm1WordAlignmentModel(\"out/VBL-WEB-IBM1/src_trg\")\n",
    "prob = model.get_translation_probability(\"es\", \"is\")\n",
    "print(f\"es -> is: {prob:.4f}\")\n",
    "prob = model.get_translation_probability(None, \"that\")\n",
    "print(f\"NULL -> that: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetrized alignment models\n",
    "\n",
    "Most statistical word alignment models are directional and asymmetric. This means that it can only model one-to-one and one-to-many alignments in one direction. They are not capable of modeling many-to-many alignments, which can occur in some language pairs. One way to get around this limitation is to train models in both directions (source-to-target and target-to-source), and then merge the resulting alignments from the two models into a single alignment. This is called symmetrization and is a common practice when using statistical word alignment models. In addition, researchers have found that symmetrized alignments are better quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine provides a special word alignment model class to support symmetrization called `ThotSymmetrizedWordAlignmentModel`. Let's demonstrate how to use this class. First, we will train the symmetrized model using the `SymmetrizedWordAlignmentModelTrainer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training direct alignment model: 0.00%\n",
      "Training direct alignment model: 0.00%\n",
      "Training direct alignment model: 8.33%\n",
      "Training direct alignment model: 16.67%\n",
      "Training direct alignment model: 25.00%\n",
      "Training direct alignment model: 33.33%\n",
      "Training direct alignment model: 41.67%\n",
      "Training direct alignment model: 50.00%\n",
      "Training inverse alignment model: 50.00%\n",
      "Training inverse alignment model: 50.00%\n",
      "Training inverse alignment model: 58.33%\n",
      "Training inverse alignment model: 66.67%\n",
      "Training inverse alignment model: 75.00%\n",
      "Training inverse alignment model: 83.33%\n",
      "Training inverse alignment model: 91.67%\n",
      "Training inverse alignment model: 100.00%\n",
      "Symmetrized IBM-1 model saved\n"
     ]
    }
   ],
   "source": [
    "from machine.translation import SymmetrizedWordAlignmentModelTrainer\n",
    "\n",
    "src_trg_trainer = ThotWordAlignmentModelTrainer(\n",
    "    ThotWordAlignmentModelType.IBM1, parallel_corpus, \"out/VBL-WEB-IBM1/src_trg\"\n",
    ")\n",
    "trg_src_trainer = ThotWordAlignmentModelTrainer(\n",
    "    ThotWordAlignmentModelType.IBM1, parallel_corpus.invert(), \"out/VBL-WEB-IBM1/trg_src\"\n",
    ")\n",
    "symmetrized_trainer = SymmetrizedWordAlignmentModelTrainer(src_trg_trainer, trg_src_trainer)\n",
    "symmetrized_trainer.train(lambda status: print(f\"{status.message}: {status.percent_completed:.2%}\"))\n",
    "symmetrized_trainer.save()\n",
    "print(\"Symmetrized IBM-1 model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can also be trained using the `create_trainer` method on `ThotSymmetrizedWordAlignmentModel`. Now that we've trained the symmetrized model, let's obtain some alignments. Machine supports many different symmetrization heuristics. The symmetrization heuristic to use when merging alignments can be specified using the `heuristic` property. In this case, we will use the `GROW_DIAG_FINAL_AND` heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1JN 1:1\n",
      "Source: esta carta trata sobre la palabra de vida que existía desde el principio , que hemos escuchado , que hemos visto con nuestros propios ojos y le hemos contemplado , y que hemos tocado con nuestras manos .\n",
      "Target: that which was from the beginning , that which we have heard , that which we have seen with our eyes , that which we saw , and our hands touched , concerning the word of life\n",
      "Alignment: 15-0 16-1 2-2 12-3 4-4 12-5 13-6 15-7 16-8 35-9 15-10 16-11 13-12 15-13 16-14 35-15 15-16 20-17 21-18 22-19 2-20 13-21 15-22 16-23 35-24 2-25 13-26 25-27 22-28 2-29 2-30 13-31 3-32 4-33 5-34 6-35 7-36\n",
      "1JN 1:2\n",
      "Source: esta vida nos fue revelada . la vimos y damos testimonio de ella . estamos hablándoles de aquél que es la vida eterna , que estaba con el padre , y que nos fue revelado .\n",
      "Target: ( and the life was revealed , and we have seen , and testify , and declare to you the life , the eternal life , which was with the father , and was revealed to us ) ;\n",
      "Alignment: 4-0 8-1 28-2 1-3 3-4 34-5 23-6 8-7 14-8 28-9 4-10 23-11 8-12 4-13 23-14 8-15 4-16 0-17 12-18 28-19 1-20 23-21 28-22 22-23 1-24 23-25 10-26 3-27 26-28 28-29 28-30 23-31 8-32 3-33 34-34 0-35 2-36 4-37 34-38\n",
      "1JN 1:3\n",
      "Source: los que hemos visto y oído eso mismo les contamos , para que también puedan participar de esta amistad junto a nosotros . esta amistad con el padre y su hijo jesucristo .\n",
      "Target: that which we have seen and heard we declare to you , that you also may have fellowship with us . yes , and our fellowship is with the father and with his son , jesus christ .\n",
      "Alignment: 2-0 2-1 2-2 2-3 3-4 4-5 18-6 2-7 18-8 6-9 8-10 10-11 2-12 8-13 13-14 14-15 2-16 18-17 25-18 21-19 22-20 5-21 10-22 4-23 18-24 18-25 26-26 25-27 27-28 27-29 4-30 25-31 29-32 30-33 10-34 31-35 31-36 22-37\n",
      "1JN 1:4\n",
      "Source: escribimos para decirles esto , a fin de que nuestra felicidad sea completa .\n",
      "Target: and we write these things to you , that our joy may be fulfilled .\n",
      "Alignment: 6-0 12-1 6-2 2-3 2-4 3-5 3-6 4-7 1-8 0-9 0-10 12-11 9-12 0-13 13-14\n",
      "1JN 1:5\n",
      "Source: este es el mensaje que recibimos de él y que nosotros les declaramos a ustedes : dios es luz , y no hay ningún vestigio de oscuridad en él .\n",
      "Target: this is the message which we have heard from him and announce to you , that god is light , and in him is no darkness at all .\n",
      "Alignment: 15-0 1-1 2-2 3-3 3-4 10-5 14-6 3-7 0-8 7-9 8-10 5-11 22-12 14-13 19-14 4-15 16-16 1-17 18-18 19-19 8-20 27-21 7-22 1-23 22-24 26-25 5-26 5-27 29-28\n"
     ]
    }
   ],
   "source": [
    "from machine.translation import SymmetrizationHeuristic\n",
    "from machine.translation.thot import ThotSymmetrizedWordAlignmentModel\n",
    "\n",
    "src_trg_model = ThotIbm1WordAlignmentModel(\"out/VBL-WEB-IBM1/src_trg\")\n",
    "trg_src_model = ThotIbm1WordAlignmentModel(\"out/VBL-WEB-IBM1/trg_src\")\n",
    "symmetrized_model = ThotSymmetrizedWordAlignmentModel(src_trg_model, trg_src_model)\n",
    "symmetrized_model.heuristic = SymmetrizationHeuristic.GROW_DIAG_FINAL_AND\n",
    "for row in islice(parallel_corpus.get_rows(), 5):\n",
    "    alignment = model.get_best_alignment(row.source_segment, row.target_segment)\n",
    "\n",
    "    print(row.ref)\n",
    "    print(\"Source:\", row.source_text)\n",
    "    print(\"Target:\", row.target_text)\n",
    "    print(\"Alignment:\", alignment)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5c3594217c843d98647b94b941817aced8bb402b4624bfdbaebf1a3617786b5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('sil-machine-hNQycPie-py3.7': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
